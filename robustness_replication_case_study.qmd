---
title: "Robustness replication of *A Bayesian Meta-Analysis of the Acoustic Properties of Infant-Directed Speech*"
subtitle: "Sampler Settings: Reanalysis of F0"
author: 
 - name: Bj√∂rn S. Siepe
   orcid: 0000-0002-9558-4648
   affiliations: University of Marburg
 - name: Matthias Kloft
   orcid: 0000-0003-1845-6957
   affiliations: University of Marburg  
 - name: Semih Can Aktepe
   orcid: 0000-0002-4776-9138
   affiliations: University of Marburg
 - name: Daniel W. Heck
   orcid: 0000-0002-6302-9252
   affiliations: University of Marburg
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
    code-fold: true
    code-tools: true
    code-summary: "Show the code"
    fig-width: 7
    fig-height: 4.5
    fig-align: "center"
    embed-resources: true
execute:
  message: false
  warning: false
  eval: true
params: 
  refit: false
---

# Preparation
In this document, we perform the case study concerning sampler settings for the $f_0$ parameter.
We first load the necessary packages and set the seed for reproducibility.
```{r setup}
library(brms)
library(tidyverse)
library(here)
library(tidybayes)
library(performance)
library(bayesplot)
library(knitr)

set.seed(35032)

# Load the data
data_F0_multiple_final <- readRDS(here("data","data_F0_multiple_final.RData"))

```


# What this Robustness Analysis is About

The authors used relatively unorthodox sampler settings by choosing an `adapt_delta` of 0.99 and a `max_treedepth` of 20. We will compare fitting the models selected as best by the authors to the same models with more standard sampler settings, as this might give us an insight into potential issues with model complexity. Also 500 warmup samples and 500 inference samples per chain (2,000 overall) should be sufficient for a well-behaved model.

The authors selected the model with language, age, task & environment as predictors as the best model. 

We fit the model with typical settings on the first dataset to diagnose what is going wrong


# Original Model with Typical Sampler Settings
```{r eval=FALSE}
#| eval: !expr params$refit


baseline_te <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Age_months + 
                    Language + Environment + Task + (1 | Language/id_site/measurement_num))

priors1 <- c(brms::prior(normal(0, 2.5), class = Intercept),
             brms::prior(normal(1, 1), class = sd),
             brms::prior(normal(0, 1), class = b),
             brms::prior(normal(0, 0.05), class = b, coef = "Age_months"),
             brms::prior(gamma(2, 0.1), class = nu))

F0_task_environment_language_age_m_sampler <- 
  brm(
    baseline_te,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final[[1]], 
    family = student,
    prior = priors1,
    iter = 1000, 
    warmup = 500,
    refresh = 1000,
    cores = 4,
    chains = 4
    )
saveRDS(F0_task_environment_language_age_m_sampler, here("output", "replication", "F0_task_environment_language_age_m_sampler.rds"))
```

```{r}
F0_task_environment_language_age_m_sampler <- 
  readRDS(here("output", "replication", "F0_task_environment_language_age_m_sampler.rds"))
```

```{r}
summary(F0_task_environment_language_age_m_sampler)
```

Two things might cause issues here:
- the `sd(Intercept)` for `Language:id_site has` a higher rhat and lower ESS than the other parameters
- The error variance is zero! We incorporate the residual in the model. At the same time, there is a random intercept for `~Language:id_site:measurement_num`

Let's check the numbers of repeated measurements for each cluster of `~Language:id_site:measurement_num`

```{r}
table(data_F0_multiple_final[[1]]$measurement_num) %>% table()
```

All clusters have only one observation. We will drop the random intercept for `~Language:id_site:measurement_num` and re-fit the model and instead incorporate the residual variance.


# Re-fit with residual variance
```{r eval=FALSE}
#| eval: !expr params$refit

model_sigma_true <-  
  bf(Effect_Size | se(Effect_Size_se, sigma = TRUE) ~ 
       1 + Age_months + Language + Environment + Task + 
       (1 | Language/id_site))

fit_sigma_true <- 
  brm(
    model_sigma_true,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final[[1]], 
    family = student,
    prior = priors1,
    iter = 1000, 
    warmup = 500,
    refresh = 1000,
    cores = 4,
    chains = 4
    )
saveRDS(fit_sigma_true, here("output", "replication", "fit_sigma_true.rds"))
```

```{r}
fit_sigma_true <- readRDS(here("output", "replication", "fit_sigma_true.rds"))
```


```{r}
summary(fit_sigma_true)
```

Rhats and ESS look better now but still there are some divergences.
Let's make the priors a little less informative and see if something breaks or the issues become more obvious. By ommiting the 'prior' argument we can use the implemented default priors.


# Re-fit with less informative priors
```{r eval=FALSE}
#| eval: !expr params$refit


model_uninformative <-  
  bf(Effect_Size | se(Effect_Size_se, sigma = TRUE) ~ 
       1 + Age_months + Language + Environment + Task + 
       (1 | Language/id_site))

fit_uninformative <-
  brm(
    model_uninformative,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final[[1]],
    family = student,
    iter = 1000,
    warmup = 500,
    refresh = 1000,
    cores = 4,
    chains = 4
  )

saveRDS(fit_uninformative, here("output", "replication", "fit_uninformative.rds"))
```

```{r}
fit_uninformative <- readRDS(here("output", "replication", "fit_uninformative.rds"))
```


Let's check the default priors.

```{r}
prior_summary(fit_uninformative) %>% as.data.frame() %>% kable()
```

```{r}
summary(fit_uninformative)
```

The model seems not to be robust to the less informative priors. The intercept and the random intercept for Language seem to be especially problematic.

Let us build the model from simple to complex to see where the divergences occur 
first and parameter estimates break.


## Intercept-only Model
```{r eval=FALSE}
#| eval: !expr params$refit


model_step_0 <-  
  bf(Effect_Size | se(Effect_Size_se, sigma = TRUE) ~ 1)

fit_step_0 <- 
  brm(
    model_step_0,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final[[1]], 
    family = student,
    iter = 1000, 
    warmup = 500,
    refresh = 1000,
    cores = 4,
    chains = 4
    )
saveRDS(fit_step_0, here("output", "replication", "fit_step_0.rds"))
```

```{r}
fit_step_0 <- readRDS(here("output", "replication", "fit_step_0.rds"))
```


```{r}
summary(fit_step_0)
```

Everything seems to be fine here. Let's add the random intercept for language.


# Model: Intercept + Random Intercept for Language
```{r eval=FALSE}
#| eval: !expr params$refit


model_step_1 <-  
  bf(Effect_Size | se(Effect_Size_se, sigma = TRUE) ~ 1 + (1 | Language))

fit_step_1 <- 
  brm(
    model_step_1,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final[[1]], 
    family = student,
    iter = 1000, 
    warmup = 500,
    refresh = 1000,
    cores = 4,
    chains = 4
    )
saveRDS(fit_step_1, here("output", "replication", "fit_step_1.rds"))
```

```{r}
fit_step_1 <- readRDS(here("output", "replication", "fit_step_1.rds"))
```

```{r}
summary(fit_step_1)
```

Everything seems to be fine here. Let's add the random intercept for site.


# Model: Intercept + Random Intercept for Language + Random Intercept for Site
```{r}
#| eval: !expr params$refit


model_step_2 <-
  bf(Effect_Size | se(Effect_Size_se, sigma = TRUE) ~
       1 + (1 | Language / id_site))

fit_step_2 <-
  brm(
    model_step_2,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final[[1]], 
    family = student,
    iter = 1000, 
    warmup = 500,
    refresh = 1000,
    cores = 4,
    chains = 4
    )
saveRDS(fit_step_2, here("output", "replication", "fit_step_2.rds"))
```

```{r}
fit_step_2 <- readRDS(here("output", "replication", "fit_step_2.rds"))
```

```{r}
summary(fit_step_2)
```

The nested random effects structure seems to be problematic as it leads to divergences. 
Let's check the frequencies of the clusters for Language and Site.


## Check Frequencies for Language and Site
### Language
Frequencies:
```{r}
freq_language <- table(data_F0_multiple_final[[1]]$Language) %>% table()
freq_language
```

Percentage of clusters with just 1 observation:
```{r}
paste0(round((freq_language[1] / sum(freq_language))*100, 1), "%")
```

Number of clusters with more than 1 observation:
```{r}
sum(freq_language[-1])
```


### Site
Frequencies:
```{r}
freq_site <- table(data_F0_multiple_final[[1]]$id_site) %>% table()
freq_site
```

Percentage of clusters with just 1 observation:
```{r}
paste0(round((freq_site[1] / sum(freq_site))*100, 1), "%")
```

Number of clusters with more than 1 observation:
```{r}
sum(freq_site[-1])
```

We have a high number of clusters with only one observation for both Language and Site. 
A model featuring random intercepts for both Language and Site is probably not identifiable.

Let's check the intra-class correlation for Language and Site to decide which random intercept to keep.

## Test intra-class correlation
```{r eval=FALSE}
#| eval: !expr params$refit


model_icc_language <-  
  bf(Effect_Size | se(Effect_Size_se, sigma = TRUE) ~ 1  + (1 | Language))

fit_icc_language <- 
  brm(
    model_icc_language,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final[[1]], 
    family = student,
    iter = 1000, 
    warmup = 500,
    refresh = 1000,
    cores = 4,
    chains = 4
    )
saveRDS(fit_icc_language, here("output", "replication", "fit_icc_language.rds"))


model_icc_site <-  
  bf(Effect_Size | se(Effect_Size_se, sigma = TRUE) ~ 1  + (1 | id_site))

fit_icc_site <- 
  brm(
    model_icc_site,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final[[1]], 
    family = student,
    iter = 1000, 
    warmup = 500,
    refresh = 1000,
    cores = 4,
    chains = 4
    )
saveRDS(fit_icc_site, here("output", "replication", "fit_icc_site.rds"))
```

```{r}
fit_icc_language <- readRDS(here("output", "replication", "fit_icc_language.rds"))
fit_icc_site <- readRDS(here("output", "replication", "fit_icc_site.rds"))
```

Language:
```{r}
icc(fit_icc_language)
```

Site:
```{r}
icc(fit_icc_site)
```

We will use Site as a random intercept and drop the random intercept for Language.
Let's add the fixed effects back in. We will first add the fixed effects for Age, Environment, and Task and leave Language out for now. Since Language has a high number of levels, we will add it last to see if it causes any issues.


# Model: Intercept + Random Intercept for Language + Fixed Effects for Covariates
```{r eval=FALSE}
#| eval: !expr params$refit

model_step_3 <-  
  bf(Effect_Size | se(Effect_Size_se, sigma = TRUE) ~ 1 + 
        Age_months + Environment + Task +
       (1 | id_site))

fit_step_3 <- 
  brm(
    model_step_3,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final[[1]], 
    family = student,
    iter = 1000, 
    warmup = 500,
    refresh = 1000,
    cores = 4,
    chains = 4
    )

saveRDS(fit_step_3, here("output", "replication", "fit_step_3.rds"))
```

```{r}
fit_step_3 <- readRDS(here("output", "replication", "fit_step_3.rds"))
print(summary(fit_step_3), digits = 2)
```

The model looks fine. Let's add Language as a fixed effect. Since we do not have random intercepts for Language, this step seems more appropriate than the approach in the original model.


# Model: Intercept + Random Intercept for Language + Fixed Effects for Age, Environment, Task, and Language
```{r eval=FALSE}
#| eval: !expr params$refit

model_step_4 <-  
  bf(Effect_Size | se(Effect_Size_se, sigma = TRUE) ~ 1 + 
        Age_months + Environment + Task + Language +
       (1 | id_site))

fit_step_4 <- 
  brm(
    model_step_4,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final[[1]], 
    family = student,
    iter = 1000, 
    warmup = 500,
    refresh = 1000,
    cores = 4,
    chains = 4
    )

saveRDS(fit_step_4, here("output", "replication", "fit_step_4.rds"))
```

```{r}
fit_step_4 <- readRDS(here("output", "replication", "fit_step_4.rds"))
print(summary(fit_step_4), digits = 2)
```

There are a lot of null effects for Language. Let's do a model comparison with the previous model, which did not include Language as a fixed effect.


# Model Comparison: Inclusion of Language as a Fixed Effect
```{r}
rbind(
No_Language = performance::model_performance(fit_step_3) %>% as.data.frame(),
Language = performance::model_performance(fit_step_4) %>% as.data.frame()
) %>% kable(digits = 3)
```
```{r}
rbind(
No_Language = performance::r2_bayes(fit_step_3) %>% as.data.frame(),
Language = performance::r2_bayes(fit_step_4) %>% as.data.frame()
) %>% kable(digits = 3)
```
Log-Bayes-Factor:
```{r}
brms::bayes_factor(fit_step_4,fit_step_3, log = TRUE)
```

The model containing Language is the better model.

Now that we have a working model, we can fit it to all datasets. We will double the sample for even better estimates.

## Fit again with multiple data sets

```{r eval=FALSE}
#| eval: !expr params$refit

fit_step_4_multiple <- 
  brm_multiple(
    model_step_4,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final, 
    family = student,
    iter = 2000, 
    warmup = 1000,
    refresh = 1000,
    cores = 4,
    chains = 4
    )

saveRDS(fit_step_4_multiple, here("output", "replication", "fit_step_4_multiple.rds"))
```

```{r}
fit_step_4_multiple <- readRDS(here("output", "replication", "fit_step_4_multiple.rds"))
```

```{r}
print(summary(fit_step_4_multiple), digits = 3)
```

```{r}
h <- c("Age_months < 0", "Environmentnaturalistic < 0", "Taskspontaneousspeech > 0")
hyp <- brms::hypothesis(
  fit_step_4_multiple,
  hypothesis = h,
  alpha = .025
)
plot(hyp)
```

```{r}
hyp[[1]] %>% select(-"CI.Lower") %>% kable(digits = 3)
```


# Compare to original model

## Refit Multiple Datasets with Original Model
```{r eval=FALSE}
#| eval: !expr params$refit


baseline_te <- bf(
  Effect_Size | se(Effect_Size_se) ~ 1 + Age_months +
    Language + Environment + Task + (1 |
                                       Language / id_site / measurement_num)
)

priors1 <- c(
  brms::prior(normal(0, 2.5), class = Intercept),
  brms::prior(normal(1, 1), class = sd),
  brms::prior(normal(0, 1), class = b),
  brms::prior(normal(0, 0.05), class = b, coef = "Age_months"),
  brms::prior(gamma(2, 0.1), class = nu)
)

F0_task_environment_language_age_m_sampler_multiple <-
  brm_multiple(
    baseline_te,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final,
    family = student,
    prior = priors1,
    iter = 2000,
    warmup = 1000,
    refresh = 1000,
    cores = 4,
    chains = 4,
    control = list(adapt_delta = 0.999, max_treedepth = 20)
  )

saveRDS(
  F0_task_environment_language_age_m_sampler_multiple,
  here(
    "output",
    "replication",
    "F0_task_environment_language_age_m_sampler_multiple.rds"
  )
)
```

```{r}
F0_task_environment_language_age_m_sampler_multiple <- 
  readRDS(here("output", "replication", "F0_task_environment_language_age_m_sampler_multiple.rds"))
```


# Compare the Original Model to Our Stripped Down Model
```{r}
h2 <- c("Age_months < 0",
        "Environmentnaturalistic < 0",
        "Taskspontaneousspeech > 0")
hyp2 <- brms::hypothesis(
  F0_task_environment_language_age_m_sampler_multiple,
  hypothesis = h2,
  alpha = .025
)
```

```{r}
rbind(Original = hyp2[[1]], Stripped = hyp[[1]]) %>%
  rownames_to_column("Model") %>% 
  mutate(Model = str_remove(Model, ".1|.2|.3")) %>% 
  arrange(Hypothesis) %>% 
  kable(digits = 3)
```

The original results hold up in our robustness analysis, although the evidence for the effects is a little bit weaker than in the original model.


# Session Info

```{r}
pander::pander(sessionInfo())
```





