---
title: "Robustness replication of *A Bayesian Meta-Analysis of the Acoustic Properties of Infant-Directed Speech*"
author: 
 - name: Bj√∂rn S. Siepe
   orcid: 0000-0002-9558-4648
   affiliations: University of Marburg
 - name: Matthias Kloft
   orcid: 0000-0003-1845-6957
   affiliations: University of Marburg  
 - name: Semih Can Aktepe
   orcid: 0000-0002-4776-9138
   affiliations: University of Marburg
 - name: Daniel W. Heck
   orcid: 0000-0002-6302-9252
   affiliations: University of Marburg
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
    code-fold: true
    code-tools: true
    code-summary: "Show the code"
    fig-width: 7
    fig-height: 4.5
    fig-align: "center"
    embed-resoureces: true
execute:
  message: false
  warning: false
  eval: false
---

# Preparation
Load the necessary packages and set the seed for reproducibility.
```{r setup}
library(brms)
library(tidyverse)
library(here)
# library(renv)
library(tidybayes)
library(RoBMA)
library(osfr)
library(bayesplot)
library(pander)
library(knitr)
library(xtable)
set.seed(35032)
``` 

# Publication Bias 
Here, we will use Robust Bayesian Meta-Analyses to fit an ensemble of meta-analytic models, which will then be averaged with Bayesian Model Averaging. These models also estimate a publication bias adjustment.
https://fbartos.github.io/RoBMA/reference/RoBMA.reg.html
## F0
The authors selected the model with language, age, task & environment as predictors as the best model. 

TODO not sure how to deal with multiple imputation yet. Maybe average

TODO how to deal with hedges g
https://forum.cogsci.nl/discussion/7239/robma-random-effects-meta-analysis-with-meta-regression

### Intercept-Only
```{r f0-intercept-robma}
data_F0 <- readRDS(here("data/data_F0_multiple_final.RData"))



# TODO hedges g instead of d
F0_pub_bias <- RoBMA(
      y = data_F0[[1]]$Effect_Size, 
      se = data_F0[[1]]$Effect_Size_se, 
      study_ids = data_F0[[1]]$id_site,
      priors_bias = NULL,
      parallel = TRUE, 
      seed = 35037)

# TODO Why is this a multivariate model? 
saveRDS(F0_pub_bias, file = here("output/replication/F0_pub_bias.RDS"))

```

Analyze the results:
```{r f0-intercept-robma-summary}
F0_pub_bias <- readRDS(here("output/replication/F0_pub_bias.RDS"))

# summary(F0_pub_bias)

# Obtain neat summary: 
interpret(F0_pub_bias)
```



## VSA
The authors selected the model with age & language as predictors as the best model.

```{r vsa-robma}

```



## AR
The authors selected the model with task, age & language as predictors as the best model.

```{r ar-robma}

```

## VD
The authors selected the model with age & language as predictors as the best model.

```{r vd-robma}

```



## F0V
The authors selected the model with task, age & language as predictors as the best model.

```{r f0v-robma}

```



# Compare Rank-Order of Model Weights

## (Re-)estimate Missing Models
First estimate the missing models, using the same prior specification as used in the models with language as predictor, but using the same random effects as in all priors. It is unclear to us why the random effects structure was changed for language. 
Environment as predictor: 
```{r}
baseline_envir <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Environment + (1 | Language/id_site/measurement_num))
priors2 <- c(prior(normal(0, 2.5), class = Intercept),
             prior(normal(1, 1), class = sd),
             prior(normal(0, 1), class = b),
             prior(gamma(2, 0.1), class = nu))

F0_environment_m <- 
  brm_multiple(
    baseline_envir,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "F0_environment_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

VSA_environment_m <- 
  brm_multiple(
    baseline_envir,
    save_pars = save_pars(all = TRUE),
    data = data_VSA_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "VSA_environment_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

AR_environment_m <- 
  brm_multiple(
    baseline_envir,
    save_pars = save_pars(all = TRUE),
    data = data_AR_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "AR_environment_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

VD_environment_m <- 
  brm_multiple(
    baseline_envir,
    save_pars = save_pars(all = TRUE),
    data = data_VD_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "VD_environment_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

pp_check(VD_environment_m, ndraws = 50)

F0V_environment_m <- 
  brm_multiple(
    baseline_envir,
    save_pars = save_pars(all = TRUE),
    data = data_F0V_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "F0V_environment_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

pp_check(F0V_language_m, ndraws = 50)
```


Task as predictor
```{r}
baseline_task <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Task + (1 | Language/id_site/measurement_num))
priors2 <- c(prior(normal(0, 2.5), class = Intercept),
             prior(normal(1, 1), class = sd),
             prior(normal(0, 1), class = b),
             prior(gamma(2, 0.1), class = nu))

F0_task_m <- 
  brm_multiple(
    baseline_task,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "F0_task_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

VSA_task_m <- 
  brm_multiple(
    baseline_task,
    save_pars = save_pars(all = TRUE),
    data = data_VSA_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "VSA_task_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

AR_task_m <- 
  brm_multiple(
    baseline_task,
    save_pars = save_pars(all = TRUE),
    data = data_AR_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "AR_task_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

VD_task_m <- 
  brm_multiple(
    baseline_task,
    save_pars = save_pars(all = TRUE),
    data = data_VD_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "VD_task_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

pp_check(VD_task_m, ndraws = 50)

F0V_task_m <- 
  brm_multiple(
    baseline_task,
    save_pars = save_pars(all = TRUE),
    data = data_F0V_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "F0V_task_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))
```


## (Re-)estimate Models with Diferent Sampling Settings
Running the code for the first time resulted in an error for AR and VD: `Each log-likelihood matrix must have the same dimensions.` 
Upon inspection, we found that some models had different dimensions, because `VD_intercept`,`AR_enviroment_language_age` and `AR_task_environment_language_age` were run with 10,000 instead of 5,000 iterations (without further explanation). We therefore re-ran these models with less iterations. 

```{r rerun-models}
# Preparations
baseline_e <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Age_months + 
                   Language + Environment + (1 | Language/id_site/measurement_num))

data_AR_multiple_final <- readRDS(here("data/data_AR_multiple_final.RData"))


baseline_z <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + (1 | Language/id_site/measurement_num))
data_VD_multiple_final <- readRDS(here("data/data_VD_multiple_final.RData"))

priors <- c(brms::prior(normal(0, 2.5), class = Intercept),
            brms::prior(normal(1, 1), class = sd),
            brms::prior(gamma(2, 0.1), class = nu))

priors1 <- c(brms::prior(normal(0, 2.5), class = Intercept),
             brms::prior(normal(1, 1), class = sd),
             brms::prior(normal(0, 1), class = b),
             brms::prior(normal(0, 0.05), class = b, coef = "Age_months"),
             brms::prior(gamma(2, 0.1), class = nu))

AR_task_environment_language_age_m <- 
  brm_multiple(
    baseline_e,
    save_pars = save_pars(all = TRUE),
    data = data_AR_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "AR_task_environment_language_age_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = 2,
    chains = 2,
    recompile = TRUE,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20 ))

AR_environment_language_age_m <- 
  brm_multiple(
    baseline_e,
    save_pars = save_pars(all = TRUE),
    data = data_AR_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "AR_environment_language_age_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20 ))

VD_intercept_m <- 
  brm_multiple(
    baseline_z,
    save_pars = save_pars(all = TRUE),
    data = data_VD_multiple_final, 
    family = student,
    prior = priors,
    file = here("models", "replication", "VD_intercept_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = 2,
    chains = 2,
    recompile = TRUE,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20 ))

```


Then, in the original code, `AR_task_language_age_m` was not estimated with `brms::brm_multiple`, but with the normal brms setting - the results therefore ignore the uncertainty of MICE imputation. Additionally, this leads to a different number of posterior samples, which impedes the calculation of model weights.

```{r}
AR_task_language_age_m <- 
  brm_multiple(
    baseline_t,
    save_pars = save_pars(all = TRUE),
    data = data_AR_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "AR_task_language_age_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))
```


We re-estimate 

## Estimate Model Weights
Now load in all relevant models to the environment.
This is including "task_language_age" and "environment_language_age" for every model, which was omitted from the original code, but mentioned in the manuscript.

```{r}
options(future.globals.maxSize = +Inf)

load_models <- function(prefix, replication = TRUE) {
  file_names <- c("environment_m_rep.rds", "task_m_rep.rds", "task_environment_language_age_m_rep.rds", "task_language_age_m_rep.rds", "environment_language_age_m_rep.rds",
                  "age_language_m_rep.rds", "age_m_rep.rds", "language_m_rep.rds", "intercept_m_rep.rds")
  # if original models instead of our replications should be used
  if(isFALSE(replication)){
    file_names <- gsub("_rep", "", file_names)
  }
  final_names <- c("environment", "task", "task_environment_language_age", "task_language_age", "environment_language_age", "age_language", "age", "language", "intercept")
  
  models <- list()
  for (i in seq_along(file_names)) {
    if(isTRUE(replication)){
      model <- readRDS(paste0("models/replication/", prefix, "_", file_names[i]))
    } else{
      model <- readRDS(paste0("models/", prefix, "_", file_names[i]))
    }
    models[[final_names[i]]] <- model
  }
  return(models)
}

# safely compute weights
compute_weights_safe <- function(models, method) {
  tryCatch({
    # unfortunately had to hardcode this, other solutions did not work 
    # as intended
    weights <- brms::model_weights(models[[1]], models[[2]], models[[3]],
                        models[[4]], models[[5]], models[[6]], 
                        models[[7]], models[[8]], models[[9]], weights = method)
    names(weights) <- names(models)
    return(weights)
  }, error = function(e) {
    message(paste("Error in model_weights with method", method, ": ", e$message))
    return(NULL)
  })
}

# compute all model weights for a given prefix
compute_model_weights <- function(prefix, pmp = FALSE) {
  models <- load_models(prefix)
  
  weights <- list(
    loo = compute_weights_safe(models, "loo"),
    stacking = compute_weights_safe(models, "stacking"),
    waic = compute_weights_safe(models, "waic"),
    if(isTRUE(pmp)){
    pmp = tryCatch({
      post_prob(models$environment, models$task, models$task_environment_language_age,
                models$task_language_age, models$environment_language_age ,models$age_language, models$age, models$language, models$intercept)
    }, error = function(e) {
      message(paste("Error in post_prob for prefix", prefix, ": ", e$message))
      return(NULL)
    })
    }
  )
  
  rm(models) # Remove models from memory after calculation
  return(weights)
}

# iterate over each prefix
prefixes <- c("F0", "VSA", "AR", "F0V", "VD")
weights <- list()


# out of curiosity: What if we exclude simpler models and only use those
# mentioned in the text?
F0_reduced_stacking <- brms::model_weights(F0_models$task_environment_language_age, 
                                           F0_models$task_language_age,
                                           F0_models$age_language,
                                           F0_models$environment_language_age)

# For now, we compute weights without pmp, which often throws errors
for(prefix in prefixes) {
  message(paste("Computing weights for prefix:", prefix))
  weights[[prefix]] <- compute_model_weights(prefix)
}

for(prefix in prefixes){
  message(paste("Computing weights for prefix:", prefix))
  models <- load_models(prefix)
  weights[[prefix]]$stacking <- compute_weights_safe(models, "stacking")
  rm(models)
}


models <- load_models("AR")
weights[["AR"]]$stacking <- compute_weights_safe(models, "stacking")
rm(models)



saveRDS(object = weights, 
        file = here("output/replication/weights_list.RDS"))

```




### Present Model Weights Results
Reload all results
```{r, eval = TRUE}
weights <- readRDS(here("output/replication/weights_list.RDS"))
```

Then create a comparison table for model rank:
```{r, eval = TRUE}
df_weights <- lapply(weights, function(x){
  bind_rows(x, .id = "weights")
}) |> 
  bind_rows(.id = "model")


# compute ranks and print to LaTeX
df_weights |> 
  pivot_longer(!c(model, weights)) |> 
  group_by(model, weights) |> 
  mutate(rank = rank(-value)) |> 
  mutate(value = round(value, 3)) |> 
  mutate(value = format(value, scientific = FALSE)) |> 
  mutate(value = paste0(value, " (", rank, ")")) |> 
  ungroup() |> 
  pivot_wider(id_cols = c(model, weights), names_from = name, values_from = value) |> 
  # prettier column names
  rename_with(~gsub("_", ", ", .)) |> 
  rename_with(str_to_title) |> 
  # Remove lower-case for shorter column names
   rename_with(~ gsub("[^A-Z]", "",.)) |> 
  rename(
    Model = M,
    Weights = W
  ) |> 
  select(Model, Weights, I, E, "T", A, L, AL, ELA, TLA, TELA) |> 
  xtable::xtable() |> 
  print(include.rownames = FALSE)
```



# Sampler Settings
The authors used relatively unorthodox sampler settings by choosing an `adapt_delta` of 0.99 and a `max_treedepth` of 20. We will compare fitting the models selected as best by the authors to the same models with more standard sampler settings, as this might give us an insight into potential issues with model complexity.

## F0
The authors selected the model with language, age, task & environment as predictors as the best model. 

```{r f0-sampler}
data_F0_multiple_final <- readRDS(here("data/data_F0_multiple_final.RData"))

baseline_te <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Age_months + 
                    Language + Environment + Task + (1 | Language/id_site/measurement_num))

priors1 <- c(brms::prior(normal(0, 2.5), class = Intercept),
             brms::prior(normal(1, 1), class = sd),
             brms::prior(normal(0, 1), class = b),
             brms::prior(normal(0, 0.05), class = b, coef = "Age_months"),
             brms::prior(gamma(2, 0.1), class = nu))

F0_task_environment_language_age_m_sampler <- 
  brm_multiple(
    baseline_te,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "sampler_check", "F0_task_environment_language_age_m_rep_sampler"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.8,
      max_treedepth = 10 ))

summary(F0_task_environment_language_age_m_sampler)
pp_check(F0_task_environment_language_age_m_sampler, ndraws = 100)

```

Now we check convergence: 
```{r}
lp_F0 <- bayesplot::log_posterior(F0_task_environment_language_age_m_sampler)
np_F0 <- bayesplot::nuts_params(F0_task_environment_language_age_m_sampler)
bayesplot::mcmc_nuts_divergence(np_F0, lp_F0)
```

```{r}
bayesplot::mcmc_pairs(F0_task_environment_language_age_m_sampler, np = np_F0, pars = c("Intercept","b_Age_months","b_LanguageBritishEnglish"),
           off_diag_args = list(size = 0.75))
```

```{r}
bayesplot::mcmc_parcoord(F0_task_environment_language_age_m_sampler, np = np_F0, pars = c("Intercept","b_Age_months","b_LanguageBritishEnglish"))
```



Compare results to the original model

## VSA
The authors selected the model with age & language as predictors as the best model.

```{r vsa-sampler}
data_VSA_multiple_final <- readRDS(here("data/data_VSA_multiple_final.RData"))

baseline_f <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + 
                   Language + Age_months + (1 | Language/id_site/measurement_num))

VSA_age_language_m_sampler <- 
  brm_multiple(
    baseline_f,
    save_pars = save_pars(all = TRUE),
    data = data_VSA_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "sampler_check", "VSA_age_language_m_rep_sampler"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.80,
      max_treedepth = 10 ))
summary(VSA_age_language_m_sampler)
pp_check(VSA_age_language_m_sampler, ndraws = 100)
```


## AR
The authors selected the model with task, age & language as predictors as the best model.

```{r ar-sampler}
data_AR_multiple_final <- readRDS(here("data/data_AR_multiple_final.RData"))
baseline_t <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Age_months + 
                   Language + Task + (1 | Language/id_site/measurement_num))

AR_task_language_age_m_sampler <- 
  brm(
    baseline_t,
    save_pars = save_pars(all = TRUE),
    data = data_AR_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "sampler_check", "AR_task_language_age_m_rep_sampler"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.80,
      max_treedepth = 10 ))

summary(AR_task_language_age_m_sampler)
pp_check(AR_task_language_age_m_sampler, ndraws = 100)

```

## VD
The authors selected the model with age & language as predictors as the best model.

```{r vd-sampler}
data_VD_multiple_final <- readRDS(here("data/data_VD_multiple_final.RData"))
baseline_f <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + 
                   Language + Age_months + (1 | Language/id_site/measurement_num))

VD_age_language_m_sampler <- 
  brm_multiple(
    baseline_f,
    save_pars = save_pars(all = TRUE),
    data = data_VD_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "sampler_check", "VD_age_language_m_rep_sampler"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.80,
      max_treedepth = 10 ))

summary(VD_age_language_m_sampler)
pp_check(VD_age_language_m_sampler, ndraws = 100)

```



## F0V
The authors selected the model with task, age & language as predictors as the best model.

```{r f0v-sampler}
data_F0V_multiple_final <- readRDS(here("data/data_F0V_multiple_final.RData"))
baseline_t <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Age_months + 
                   Language + Task + (1 | Language/id_site/measurement_num))
priors1 <- c(brms::prior(normal(0, 2.5), class = Intercept),
             brms::prior(normal(1, 1), class = sd),
             brms::prior(normal(0, 1), class = b),
             brms::prior(normal(0, 0.05), class = b, coef = "Age_months"),
             brms::prior(gamma(2, 0.1), class = nu))

F0V_task_language_age_m_sampler <- 
  brm_multiple(
    baseline_t,
    save_pars = save_pars(all = TRUE),
    data = data_F0V_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "sampler_check", "F0V_task_language_age_m_rep_sampler_new"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = 2,
    recompile = TRUE,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.80,
      max_treedepth = 10 ))

summary(F0V_task_language_age_m_sampler)
pp_check(F0V_task_language_age_m_sampler, ndraws = 100)

```

We compare point estimates to the original model (i.e., or replication of it, because the original one was not available):

```{r f0v-comparison}
# load original model
F0V_task_langugae_age_m_rep <- readRDS("~/ids-meta-replication/models/replication/F0V_task_language_age_m_rep.rds")

source("functions.R")
diff_F0V <- diff_point_est(F0V_task_language_age_m_rep, F0V_task_language_age_m_sampler)

```


Now we check convergence: 
```{r}
lp_F0V <- bayesplot::log_posterior(F0V_task_language_age_m_sampler)
np_F0V <- bayesplot::nuts_params(F0V_task_language_age_m_sampler)
bayesplot::mcmc_nuts_divergence(np_F0V, lp_F0V)
```

```{r}
bayesplot::mcmc_parcoord(F0V_task_language_age_m_sampler, np = np_F0V, pars = c("Intercept","b_Age_months","b_LanguageBritishEnglish"))
```

```{r}
bayesplot::mcmc_pairs(F0V_task_language_age_m_sampler, np = np_F0V, pars = c("Intercept","nu"))
```






# Session Info

```{r}
pander::pander(sessionInfo())
```





