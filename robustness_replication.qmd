---
title: "Robustness replication of *A Bayesian Meta-Analysis of the Acoustic Properties of Infant-Directed Speech*"
subtitle: "Publication Bias, Model Weights, and Sampler Settings"
author: 
 - name: Bj√∂rn S. Siepe
   orcid: 0000-0002-9558-4648
   affiliations: University of Marburg
 - name: Matthias Kloft
   orcid: 0000-0003-1845-6957
   affiliations: University of Marburg  
 - name: Semih Can Aktepe
   orcid: 0000-0002-4776-9138
   affiliations: University of Marburg
 - name: Daniel W. Heck
   orcid: 0000-0002-6302-9252
   affiliations: University of Marburg
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
    code-fold: true
    code-tools: true
    code-summary: "Show the code"
    fig-width: 7
    fig-height: 4.5
    fig-align: "center"
    embed-resources: true
execute:
  message: false
  warning: false
  eval: false
---

# Preparation
Load the necessary packages and set the seed for reproducibility.
```{r setup, eval = TRUE}
library(brms)
library(tidyverse)
library(here)
# library(renv)
library(tidybayes)
library(RoBMA)
library(osfr)
library(bayesplot)
library(pander)
library(knitr)
library(xtable)
set.seed(35032)
source("functions.R")
``` 

# Publication Bias 
Here, we intended to  use Robust Bayesian Meta-Analyses to fit an ensemble of meta-analytic models, which would then be averaged with Bayesian Model Averaging. These models also include a publication bias adjustment.
https://fbartos.github.io/RoBMA/reference/RoBMA.reg.html


We used an auxiliary function to estimate all model-averaged models. However, these analyses did not converge, so we just include the code for transparency.
```{r}
prefixes <- c("F0", "VSA", "AR", "F0V", "VD")
robma_fits <- list()

for(prefix in prefixes){
  message(paste("Computing RoBMA for prefix:", prefix))
  # account for negative expected effect for AR
  if(prefix != "AR"){
    robma_fits[[prefix]] <- estimate_robma(prefix,
                                         direction = "positive")
  } else {
        robma_fits[[prefix]] <- estimate_robma(prefix,
                                         direction = "negative")
  }

}

```



# Compare Rank-Order of Model Weights

## (Re-)estimate Missing Models
First estimate the missing models, using the same prior specification as used in the models with language as predictor, but using the same random effects as in all priors. It is unclear to us why the random effects structure was changed for language. 
Environment as predictor: 
```{r}
baseline_envir <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Environment + (1 | Language/id_site/measurement_num))
priors2 <- c(prior(normal(0, 2.5), class = Intercept),
             prior(normal(1, 1), class = sd),
             prior(normal(0, 1), class = b),
             prior(gamma(2, 0.1), class = nu))

F0_environment_m <- 
  brm_multiple(
    baseline_envir,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "F0_environment_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

VSA_environment_m <- 
  brm_multiple(
    baseline_envir,
    save_pars = save_pars(all = TRUE),
    data = data_VSA_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "VSA_environment_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

AR_environment_m <- 
  brm_multiple(
    baseline_envir,
    save_pars = save_pars(all = TRUE),
    data = data_AR_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "AR_environment_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

VD_environment_m <- 
  brm_multiple(
    baseline_envir,
    save_pars = save_pars(all = TRUE),
    data = data_VD_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "VD_environment_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

pp_check(VD_environment_m, ndraws = 50)

F0V_environment_m <- 
  brm_multiple(
    baseline_envir,
    save_pars = save_pars(all = TRUE),
    data = data_F0V_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "F0V_environment_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

pp_check(F0V_language_m, ndraws = 50)
```


Task as predictor
```{r}
baseline_task <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Task + (1 | Language/id_site/measurement_num))
priors2 <- c(prior(normal(0, 2.5), class = Intercept),
             prior(normal(1, 1), class = sd),
             prior(normal(0, 1), class = b),
             prior(gamma(2, 0.1), class = nu))

F0_task_m <- 
  brm_multiple(
    baseline_task,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "F0_task_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

VSA_task_m <- 
  brm_multiple(
    baseline_task,
    save_pars = save_pars(all = TRUE),
    data = data_VSA_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "VSA_task_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

AR_task_m <- 
  brm_multiple(
    baseline_task,
    save_pars = save_pars(all = TRUE),
    data = data_AR_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "AR_task_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

VD_task_m <- 
  brm_multiple(
    baseline_task,
    save_pars = save_pars(all = TRUE),
    data = data_VD_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "VD_task_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))

pp_check(VD_task_m, ndraws = 50)

F0V_task_m <- 
  brm_multiple(
    baseline_task,
    save_pars = save_pars(all = TRUE),
    data = data_F0V_multiple_final, 
    family = student,
    prior = priors2,
    file = here("models", "replication", "F0V_task_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))
```


## (Re-)estimate Models with Diferent Sampling Settings
Running the code for the first time resulted in an error for AR and VD: `Each log-likelihood matrix must have the same dimensions.` 
Upon inspection, we found that some models had different dimensions, because `VD_intercept`,`AR_enviroment_language_age` and `AR_task_environment_language_age` were run with 10,000 instead of 5,000 iterations (without further explanation). We therefore re-ran these models with less iterations. 

```{r rerun-models}
# Preparations
baseline_e <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Age_months + 
                   Language + Environment + (1 | Language/id_site/measurement_num))

data_AR_multiple_final <- readRDS(here("data/data_AR_multiple_final.RData"))


baseline_z <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + (1 | Language/id_site/measurement_num))
data_VD_multiple_final <- readRDS(here("data/data_VD_multiple_final.RData"))

priors <- c(brms::prior(normal(0, 2.5), class = Intercept),
            brms::prior(normal(1, 1), class = sd),
            brms::prior(gamma(2, 0.1), class = nu))

priors1 <- c(brms::prior(normal(0, 2.5), class = Intercept),
             brms::prior(normal(1, 1), class = sd),
             brms::prior(normal(0, 1), class = b),
             brms::prior(normal(0, 0.05), class = b, coef = "Age_months"),
             brms::prior(gamma(2, 0.1), class = nu))

AR_task_environment_language_age_m <- 
  brm_multiple(
    baseline_e,
    save_pars = save_pars(all = TRUE),
    data = data_AR_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "AR_task_environment_language_age_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = 2,
    chains = 2,
    recompile = TRUE,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20 ))

AR_environment_language_age_m <- 
  brm_multiple(
    baseline_e,
    save_pars = save_pars(all = TRUE),
    data = data_AR_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "AR_environment_language_age_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20 ))

VD_intercept_m <- 
  brm_multiple(
    baseline_z,
    save_pars = save_pars(all = TRUE),
    data = data_VD_multiple_final, 
    family = student,
    prior = priors,
    file = here("models", "replication", "VD_intercept_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = 2,
    chains = 2,
    recompile = TRUE,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20 ))

```

Then, in the original code, `AR_task_language_age_m` was seemingly not estimated with `brms::brm_multiple`, but with the normal brms setting - the results therefore ignore the uncertainty of MICE imputation. Additionally, this leads to a different number of posterior samples, which impedes the calculation of model weights.

```{r}
AR_task_language_age_m <- 
  brm_multiple(
    baseline_t,
    save_pars = save_pars(all = TRUE),
    data = data_AR_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "AR_task_language_age_m_rep"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20 ))
```


We re-estimate 

## Estimate Model Weights
Now load in all relevant models to the environment.
This is including "task_language_age" and "environment_language_age" for every model, which was omitted from the original code, but mentioned in the manuscript.

```{r}
options(future.globals.maxSize = +Inf)


# iterate over each prefix
prefixes <- c("F0", "VSA", "AR", "F0V", "VD")
weights <- list()


# out of curiosity: What if we exclude simpler models and only use those
# mentioned in the text?
F0_reduced_stacking <- brms::model_weights(F0_models$task_environment_language_age, 
                                           F0_models$task_language_age,
                                           F0_models$age_language,
                                           F0_models$environment_language_age)

# For now, we compute weights without pmp, which often throws errors
for(prefix in prefixes) {
  message(paste("Computing weights for prefix:", prefix))
  weights[[prefix]] <- compute_model_weights(prefix)
}




saveRDS(object = weights, 
        file = here("output/replication/weights_list.RDS"))

```




### Present Model Weights Results
Reload all results
```{r, eval = TRUE}
weights <- readRDS(here("output/replication/weights_list.RDS"))
```

Then create a comparison table for model rank for the paper:
```{r, eval = FALSE}
df_weights <- lapply(weights, function(x){
  bind_rows(x, .id = "weights")
}) |> 
  bind_rows(.id = "model")


# compute ranks and print to LaTeX
df_weights |> 
  pivot_longer(!c(model, weights)) |> 
  group_by(model, weights) |> 
  mutate(rank = rank(-value)) |> 
  mutate(value = round(value, 3)) |> 
  mutate(value = format(value, scientific = FALSE)) |> 
  mutate(value = paste0(value, " (", rank, ")")) |> 
  ungroup() |> 
  pivot_wider(id_cols = c(model, weights), names_from = name, values_from = value) |> 
  # prettier column names
  rename_with(~gsub("_", ", ", .)) |> 
  rename_with(str_to_title) |> 
  # Remove lower-case for shorter column names
   rename_with(~ gsub("[^A-Z]", "",.)) |> 
  rename(
    Model = M,
    Weights = W
  ) |> 
  select(Model, Weights, I, E, "T", A, L, AL, ELA, TLA, TELA) |> 
  xtable::xtable() |> 
  print(include.rownames = FALSE)
```



# Sampler Settings
The authors used relatively unorthodox sampler settings by choosing an `adapt_delta` of 0.99 and a `max_treedepth` of 20. We will compare fitting the models selected as best by the authors to the same models with more standard sampler settings, as this might give us an insight into potential issues with model complexity. 
We provide a deeper example investigation of convergence issues for FO in another supplementary file. 

## F0
The authors selected the model with language, age, task & environment as predictors as the best model. 

```{r f0-sampler}
data_F0_multiple_final <- readRDS(here("data/data_F0_multiple_final.RData"))

baseline_te <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Age_months + 
                    Language + Environment + Task + (1 | Language/id_site/measurement_num))

priors1 <- c(brms::prior(normal(0, 2.5), class = Intercept),
             brms::prior(normal(1, 1), class = sd),
             brms::prior(normal(0, 1), class = b),
             brms::prior(normal(0, 0.05), class = b, coef = "Age_months"),
             brms::prior(gamma(2, 0.1), class = nu))

F0_task_environment_language_age_m_sampler <- 
  brm_multiple(
    baseline_te,
    save_pars = save_pars(all = TRUE),
    data = data_F0_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "sampler_check", "F0_task_environment_language_age_m_rep_sampler"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.8,
      max_treedepth = 10 ))



```



### Compare Point Estimates
We now compare the point estimates of the re-estimated model with those of the original model:
```{r, eval = TRUE}
F0_task_environment_language_age_m <- readRDS("models/F0_task_environment_language_age_m.rds")
F0_task_environment_language_age_m_sampler <- readRDS("models/replication/sampler_check/F0_task_environment_language_age_m_rep_sampler.rds")

fit_compare <- diff_point_est(F0_task_environment_language_age_m,
                   F0_task_environment_language_age_m_sampler)

fit_compare |> 
  mutate(across(contains("Diff"),
                ~round(.x, 3))) |> 
  knitr::kable()
```


## VSA
The authors selected the model with age & language as predictors as the best model.

```{r vsa-sampler}
data_VSA_multiple_final <- readRDS(here("data/data_VSA_multiple_final.RData"))

baseline_f <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + 
                   Language + Age_months + (1 | Language/id_site/measurement_num))

VSA_age_language_m_sampler <- 
  brm_multiple(
    baseline_f,
    save_pars = save_pars(all = TRUE),
    data = data_VSA_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "sampler_check", "VSA_age_language_m_rep_sampler"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.80,
      max_treedepth = 10 ))
summary(VSA_age_language_m_sampler)
pp_check(VSA_age_language_m_sampler, ndraws = 100)
```



### Compare Point Estimates
We now compare the point estimates of the re-estimated model with those of the original model (or rather, our replication of it):
```{r, eval = TRUE}
VSA_age_language_m <- readRDS("models/replication/VSA_age_language_m_rep.rds")
VSA_age_language_m_sampler <- readRDS("models/replication/sampler_check/VSA_age_language_m_rep_sampler.rds")

fit_compare <- diff_point_est(VSA_age_language_m ,
                   VSA_age_language_m_sampler)

fit_compare |> 
  mutate(across(contains("Diff"),
                ~round(.x, 3))) |> 
  knitr::kable()
```


## AR
The authors selected the model with task, age & language as predictors as the best model.

```{r ar-sampler}
data_AR_multiple_final <- readRDS(here("data/data_AR_multiple_final.RData"))
baseline_t <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Age_months + 
                   Language + Task + (1 | Language/id_site/measurement_num))

AR_task_language_age_m_sampler <- 
  brm(
    baseline_t,
    save_pars = save_pars(all = TRUE),
    data = data_AR_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "sampler_check", "AR_task_language_age_m_rep_sampler"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.80,
      max_treedepth = 10 ))

summary(AR_task_language_age_m_sampler)
pp_check(AR_task_language_age_m_sampler, ndraws = 100)

```


```{r, eval = TRUE}
AR_task_language_age_m <- readRDS("models/AR_task_language_age_m.rds")
AR_task_language_age_m_sampler <- readRDS("models/replication/sampler_check/AR_task_language_age_m_rep_sampler.rds")

fit_compare <- diff_point_est(AR_task_language_age_m,
                   AR_task_language_age_m_sampler)

fit_compare |> 
  mutate(across(contains("Diff"),
                ~round(.x, 3))) |> 
  knitr::kable()
```

## VD
The authors selected the model with age & language as predictors as the best model.

```{r vd-sampler}
data_VD_multiple_final <- readRDS(here("data/data_VD_multiple_final.RData"))
baseline_f <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + 
                   Language + Age_months + (1 | Language/id_site/measurement_num))

VD_age_language_m_sampler <- 
  brm_multiple(
    baseline_f,
    save_pars = save_pars(all = TRUE),
    data = data_VD_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "sampler_check", "VD_age_language_m_rep_sampler"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = cores,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.80,
      max_treedepth = 10 ))

summary(VD_age_language_m_sampler)
pp_check(VD_age_language_m_sampler, ndraws = 100)

```


Need to use the replicated model again: 
```{r, eval = TRUE}
VD_age_language_m <- readRDS("models/replication/VD_age_language_m_rep.rds")
VD_age_language_m_sampler <- readRDS("models/replication/sampler_check/VD_age_language_m_rep_sampler.rds")

fit_compare <- diff_point_est(VD_age_language_m,
                   VD_age_language_m_sampler)

fit_compare |> 
  mutate(across(contains("Diff"),
                ~round(.x, 3))) |> 
  knitr::kable()
```

## F0V
The authors selected the model with task, age & language as predictors as the best model.

```{r f0v-sampler}
data_F0V_multiple_final <- readRDS(here("data/data_F0V_multiple_final.RData"))
baseline_t <- bf(Effect_Size | se(Effect_Size_se) ~ 1 + Age_months + 
                   Language + Task + (1 | Language/id_site/measurement_num))
priors1 <- c(brms::prior(normal(0, 2.5), class = Intercept),
             brms::prior(normal(1, 1), class = sd),
             brms::prior(normal(0, 1), class = b),
             brms::prior(normal(0, 0.05), class = b, coef = "Age_months"),
             brms::prior(gamma(2, 0.1), class = nu))

F0V_task_language_age_m_sampler <- 
  brm_multiple(
    baseline_t,
    save_pars = save_pars(all = TRUE),
    data = data_F0V_multiple_final, 
    family = student,
    prior = priors1,
    file = here("models", "replication", "sampler_check", "F0V_task_language_age_m_rep_sampler"),
    sample_prior = T,
    iter = 5000, 
    warmup = 500,
    cores = 2,
    recompile = TRUE,
    chains = 2,
    #backend = "cmdstanr",
    #threads = threading(2),
    control = list(
      adapt_delta = 0.80,
      max_treedepth = 10 ))

summary(F0V_task_language_age_m_sampler)
pp_check(F0V_task_language_age_m_sampler, ndraws = 100)

```

We compare point estimates to the original model (i.e., or replication of it, because the original one was not available):
```{r f0v-comparison, eval = TRUE}
# load original model
F0V_task_language_age_m_rep <- readRDS("models/replication/F0V_task_language_age_m_rep.rds")
F0V_task_language_age_m_sampler<- readRDS("models/replication/sampler_check/F0V_task_language_age_m_rep_sampler.rds")



fit_compare <- diff_point_est(F0V_task_language_age_m_rep, F0V_task_language_age_m_sampler)
fit_compare |> 
  mutate(across(contains("Diff"),
                ~round(.x, 3))) |> 
  knitr::kable()
```





# Session Info

```{r}
pander::pander(sessionInfo())
```





